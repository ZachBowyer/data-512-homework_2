{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4940cc4",
   "metadata": {},
   "source": [
    "# This notebook serves as the executable software that reads in datasets, combines datasets, and runs data analysis\n",
    "\n",
    "The end goal is to perform analysis on how the coverage of US cities on wikipedia and how the quality of articles about cities varies among states. \n",
    "\n",
    "Steps: \n",
    "1. Confirm datasets are ready to go.\n",
    "2. Get article quality predictions\n",
    "3. Combine dataset of wikipedia articles with a dataset of state populations\n",
    "4. Data analyis  \n",
    "   3a. The states with the greatest and least coverage of cities on Wikipedia compared to their population.  \n",
    "   3b. The states with the highest and lowest proportion of high quality articles about cities.  \n",
    "   3c. A ranking of US geographic regions by articles-per-person and proportion of high quality articles.  \n",
    "5. Print results - 6 tables of various state/region per capita article/article quality counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7490d2",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5558f155",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import json\n",
    "import time\n",
    "import urllib.parse\n",
    "import requests\n",
    "import base64\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838c9cee",
   "metadata": {},
   "source": [
    "# Part 1. Confirm datasets are ready to go. \n",
    "\n",
    "Confirm that the files below are there:  \n",
    "./data/PopulationEstimates.csv   \n",
    "./data/States_by_region.csv  \n",
    "./data/us_cities_by_state_SEPT2023.csv  \n",
    "\n",
    "#### Step 1A: Read in ./data/PopulationEstimates.csv as csv object.\n",
    "    ##### Step 1A1: Store only state and population data from csv into list for just states. \n",
    "    ####  Step 1A2: Output first, middle, and last row of csv object to confirm data is in memory. \n",
    "#### Step 1B: Read in ./data/States_by_region.csv as csv object.\n",
    "    ##### Step 1B1: Store all data from csv into list. \n",
    "    ##### Step 1B2: Remove header row from list to prevent it showing up later.    \n",
    "    ##### Step 1B3: Output first, middle, and last row of csv object to confirm data is in memory. \n",
    "#### Step 1C: Read in ./data/us_cities_by_state_SEPT2023.csv as csv object.\n",
    "    ##### Step 1C1: Store all data from csv into list. \n",
    "    ##### Step 1C2: Remove header row from list to prevent it showing up later.    \n",
    "    ##### Step 1C3: Output first, middle, and last row of csv object to confirm data is in memory. \n",
    "\n",
    "### Data sources: \n",
    "##### ./data/PopulationEstimates.csv  \n",
    "https://www2.census.gov/programs-surveys/popest/datasets/2020-2022/state/totals/NST-EST2022-ALLDATA.csv    \n",
    "https://www.census.gov/data/tables/time-series/demo/popest/2020s-state-total.html  \n",
    "##### ./data/States_by_region.csv    \n",
    "The shared google drive for Homework 2 by UW Data 512 provides this file.   \n",
    "##### ./data/us_cities_by_state_SEPT2023.csv  \n",
    "The shared google drive for Homework 2 by UW data 512 provies this file.   \n",
    "However the Wikipedia Category:Lists of cities in the United States by   \n",
    "state was crawled to generate a list of Wikipedia article pages about US  \n",
    "cities from each state.   \n",
    "\n",
    "# Data issues:\n",
    "I did not find any inconsistencies like the HW2 assignment suggested I might here.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6b54a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of states, should be 50: 50\n",
      "['Alabama', '5074296']\n",
      "['Montana', '1122867']\n",
      "['Wyoming', '581381']\n"
     ]
    }
   ],
   "source": [
    "# Step 1A: Read in ./data/PopulationEstimates.csv as csv object.\n",
    "StatePopEstimates = []\n",
    "RegionalPopEstimates = []\n",
    "with open('../data/PopulationEstimates.csv', newline='') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "    \n",
    "    # Step 1A1: Store only state and population data from csv into list for just states. \n",
    "    #  This means excluding the first 15 and the last rows, which correspond to \n",
    "    #  the header row, larger regions of the United States, and Puerto Rico, which are not states. \n",
    "    counter = 0\n",
    "    for row in reader: \n",
    "        if(counter > 14 and counter < 66):\n",
    "            StatePopEstimates.append([row[4], row[8]])\n",
    "        elif(counter < 15):\n",
    "            RegionalPopEstimates.append([row[4], row[8]])\n",
    "        counter += 1\n",
    "        \n",
    "#Remove header, US population, Washington DC\n",
    "RegionalPopEstimates.pop(0)\n",
    "RegionalPopEstimates.pop(0)\n",
    "StatePopEstimates.pop(8)\n",
    "        \n",
    "# Step 1A2: Output first, middle, and last row of csv object to confirm data is in memory.  \n",
    "print(\"Number of states, should be 50:\", len(StatePopEstimates))\n",
    "print(StatePopEstimates[0])\n",
    "print(StatePopEstimates[len(StatePopEstimates)//2])\n",
    "print(StatePopEstimates[len(StatePopEstimates)-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bdae947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of states, should be 50: 50\n",
      "['Northeast', 'New England', 'Connecticut']\n",
      "['South', 'South Atlantic', 'North Carolina']\n",
      "['West', 'Pacific', 'Washington']\n"
     ]
    }
   ],
   "source": [
    "# Step 1B: Read in ./data/States_by_region.csv as csv object.\n",
    "StateRegions = []\n",
    "with open('../data/States_by_region.csv', newline='') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "    \n",
    "    # Step 1B1: Store all data from csv into list. \n",
    "    for row in reader: \n",
    "        StateRegions.append([row[0], row[1], row[2]]) \n",
    "\n",
    "# Step 1B2: Remove header row\n",
    "StateRegions.pop(0)\n",
    "\n",
    "# Step 1B3: Output first, middle, and last row of csv object to confirm data is in memory.\n",
    "print(\"Number of states, should be 50:\", len(StateRegions))\n",
    "print(StateRegions[0])\n",
    "print(StateRegions[len(StateRegions)//2])\n",
    "print(StateRegions[len(StateRegions)-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ac7d1f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Alabama', 'Abbeville, Alabama', 'https://en.wikipedia.org/wiki/Abbeville,_Alabama']\n",
      "['Minnesota', 'Sargeant, Minnesota', 'https://en.wikipedia.org/wiki/Sargeant,_Minnesota']\n",
      "['Wyoming', 'Yoder, Wyoming', 'https://en.wikipedia.org/wiki/Yoder,_Wyoming']\n"
     ]
    }
   ],
   "source": [
    "#### Step 1C: Read in ./data/us_cities_by_state_SEPT2023.csv as csv object.\n",
    "CityArticles = []\n",
    "with open('../data/us_cities_by_state_SEPT2023.csv', newline='', encoding=\"utf8\") as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "    \n",
    "    # Step 1C1: Store all data from csv into list. \n",
    "    for row in reader: \n",
    "        CityArticles.append([row[0], row[1], row[2]]) \n",
    "\n",
    "# Step 1C2: Remove header row\n",
    "CityArticles.pop(0)\n",
    "\n",
    "# Step 1C3: Output first, middle, and last row of csv object to confirm data is in memory.\n",
    "print(CityArticles[0])\n",
    "print(CityArticles[len(CityArticles)//2])\n",
    "print(CityArticles[len(CityArticles)-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7802a3",
   "metadata": {},
   "source": [
    "# Part 2. Get Article Quality Predictions\n",
    "We're using a machine learning system called ORES. This was originally an acronym for \"Objective Revision Evaluation Service\" but was simply renamed “ORES”. ORES is a machine learning tool that can provide estimates of Wikipedia article quality. The article quality estimates are, from best to worst:\n",
    "FA - Featured article\n",
    "GA - Good article (sometimes called A-class)\n",
    "B - B-class article\n",
    "C - C-class article\n",
    "Start - Start-class article\n",
    "Stub - Stub-class article\n",
    "These labelings were learned based on articles in Wikipedia that were peer-reviewed using the Wikipedia content assessment procedures.These quality classes are a subset of quality assessment categories developed by Wikipedia editors.\n",
    "\n",
    "#### Step 2A: Create wikimedia user account to generate API token. \n",
    "    ##### Step 2A1: Create account here: https://api.wikimedia.org/w/index.php?title=Special:UserLogin \n",
    "    ##### Step 2A2: Then go here to create token: https://api.wikimedia.org/wiki/Special:AppManagement\n",
    "    ##### Step 2A3: Click Create key, choosen personal API token, checkmark all permsissions\n",
    "#### Step 2B: Define constants/functions to make data requests.\n",
    "    ##### Step 2B1: Create function to load in credentials from text file which is not allowed to be pushed to the repo.\n",
    "    ##### Step 2B2: Load in credentials. \n",
    "    ##### Step 2B3: Define functions and constants for pageinfo API. \n",
    "    ##### Step 2B4: Define functions and constants for ORES API. \n",
    "#### Step 2C: Get ORES score for each city article. \n",
    "    ##### Step 2D1: Read each line of us_cities_by_state_SEPT.2023.csv\n",
    "    ##### Step 2D2: Make a page info request to get the current article page revision\n",
    "    ##### Step 2D3: Make an ORES request using the page title and current revision id.\n",
    "    ##### Step 2D4: Store score predictions if possible, else print out failures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "931d1013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Credentials loaded successfully:\n"
     ]
    }
   ],
   "source": [
    "# Step 2B1: Create function to load in credentials from text file which is not allowed to be pushed to the repository.\n",
    "def load_credentials_from_file(filename):\n",
    "    '''\n",
    "    Description:\n",
    "        Given a text file with six lines, where the second, fourth, and sixth lines are\n",
    "        respectively client id, client secret acess token, and acess token, reads the text file\n",
    "        and loads those lines into memory as variables. \n",
    "    Inputs:\n",
    "        filename - String - Path of file\n",
    "    Outputs:\n",
    "        client_id - String\n",
    "        client_secret - String\n",
    "        access_token - String\n",
    "    Notes:\n",
    "        This function below was generated by chatGPT. https://chat.openai.com/ \n",
    "        Prompt used:\n",
    "            \"Given a text file with 6 lines, where the 2nd, 4th, and 6th lines \n",
    "             are a client id, client secret access token, and access token,\n",
    "             give me a python function that that reads in a text file and loads \n",
    "             those lines into memory as variables. Make sure to close the file.\"\n",
    "        Why: \n",
    "            Wanted to save time, knew how to explain the problem, can easily make this myself, simple code to generate.\n",
    "    '''\n",
    "    try:\n",
    "        with open(filename, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "        if len(lines) >= 6:\n",
    "            client_id = lines[1].strip()\n",
    "            client_secret = lines[3].strip()\n",
    "            access_token = lines[5].strip()\n",
    "            return client_id, client_secret, access_token\n",
    "        else:\n",
    "            print(\"Error: The file does not contain enough lines.\")\n",
    "            return None, None, None\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{filename}' not found.\")\n",
    "        return None, None, None\n",
    "    except Exception as e:\n",
    "        print(f\"Error: An unexpected error occurred - {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "# Step 2B2: Load in credentials\n",
    "filename = '../auth.txt'\n",
    "client_id, client_secret, access_token = load_credentials_from_file(filename)\n",
    "if client_id is not None and client_secret is not None and access_token is not None:\n",
    "    print(\"Credentials loaded successfully:\")\n",
    "else:\n",
    "    print(\"Failed to load credentials.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e796bc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2B3: Define functions and constants for pageinfo API. \n",
    "\n",
    "# The code in this cell was developed by Dr. David W. McDonald for use in DATA 512, \n",
    "# a course in the UW MS Data Science degree program. \n",
    "# This code is provided under the Creative Commons CC-BY license. Revision 1.1 - August 14, 2022\n",
    "# Any modifications made to the original source also fall under the CC-BY license. \n",
    "\n",
    "#    The throttling rate is a function of the Access token that you are granted when you request the token. The constants\n",
    "#    come from dissecting the token and getting the rate limits from the granted token. \n",
    "API_LATENCY_ASSUMED = 0.002       # Assuming roughly 2ms latency on the API and network\n",
    "API_THROTTLE_WAIT = (60.0/5000.0)-API_LATENCY_ASSUMED\n",
    "\n",
    "# The basic English Wikipedia API endpoint\n",
    "API_ENWIKIPEDIA_ENDPOINT = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "# When making automated requests we should include something that is unique to the person making the request\n",
    "REQUEST_HEADERS = {\n",
    "    'User-Agent': '<uwnetid@uw.edu>, University of Washington, MSDS DATA 512 - AUTUMN 2023',\n",
    "}\n",
    "\n",
    "# This is a string of additional page properties that can be returned see the Info documentation for what can be included.\n",
    "PAGEINFO_EXTENDED_PROPERTIES = \"talkid|url|watched|watchers\"\n",
    "\n",
    "# This template lists the basic parameters for making this\n",
    "PAGEINFO_PARAMS_TEMPLATE = { \"action\": \"query\", \"format\": \"json\", \"titles\": \"\", \"prop\": \"info\",\n",
    "                            \"inprop\": PAGEINFO_EXTENDED_PROPERTIES\n",
    "}\n",
    "\n",
    "def request_pageinfo_per_article(article_title = None, \n",
    "                                 endpoint_url = API_ENWIKIPEDIA_ENDPOINT, \n",
    "                                 request_template = PAGEINFO_PARAMS_TEMPLATE,\n",
    "                                 headers = REQUEST_HEADERS):\n",
    "    '''\n",
    "    Description:\n",
    "        Make request to endpoint with supplied arguments.\n",
    "    Inputs:\n",
    "        article_title - String\n",
    "        endpoint_url - String\n",
    "        request_template - Dictionary\n",
    "        headers - Dictionary\n",
    "    Output:\n",
    "        Dictionary\n",
    "    '''\n",
    "    \n",
    "    # article title can be as a parameter to the call or in the request_template\n",
    "    if article_title:\n",
    "        request_template['titles'] = article_title\n",
    "\n",
    "    if not request_template['titles']:\n",
    "        raise Exception(\"Must supply an article title to make a pageinfo request.\")\n",
    "\n",
    "    # make the request\n",
    "    try:\n",
    "        # we'll wait first, to make sure we don't exceed the limit in the situation where an exception\n",
    "        # occurs during the request processing - throttling is always a good practice with a free\n",
    "        # data source like Wikipedia - or any other community sources\n",
    "        if API_THROTTLE_WAIT > 0.0:\n",
    "            time.sleep(API_THROTTLE_WAIT)\n",
    "        response = requests.get(endpoint_url, headers=headers, params=request_template)\n",
    "        json_response = response.json()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        json_response = None\n",
    "    return json_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "013a78f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2B4: Define functions and constants for ORES API. \n",
    "\n",
    "# The code in this cell was developed by Dr. David W. McDonald for use in DATA 512, \n",
    "# a course in the UW MS Data Science degree program. \n",
    "# This code is provided under the Creative Commons CC-BY license. Revision 1.1 - August 15, 2023\n",
    "# Any modifications made to the original source also fall under the CC-BY license. \n",
    "\n",
    "# The current LiftWing ORES API endpoint and prediction model\n",
    "API_ORES_LIFTWING_ENDPOINT = \"https://api.wikimedia.org/service/lw/inference/v1/models/{model_name}:predict\"\n",
    "API_ORES_EN_QUALITY_MODEL = \"enwiki-articlequality\"\n",
    "\n",
    "# When making automated requests we should include something that is unique to the person making the request\n",
    "# This should include an email - your UW email would be good to put in there   \n",
    "# Because all LiftWing API requests require some form of authentication, you need to provide your access token\n",
    "# as part of the header too\n",
    "REQUEST_HEADER_TEMPLATE = {\n",
    "    'User-Agent': \"<zbowyer@uw.edu>, University of Washington, MSDS DATA 512 - AUTUMN 2023\",\n",
    "    'Content-Type': 'application/json',\n",
    "    'Authorization': \"Bearer )\" + str(access_token)\n",
    "}\n",
    "\n",
    "#    This is a template for the parameters that we need to supply in the headers of an API request\n",
    "REQUEST_HEADER_PARAMS_TEMPLATE = {\n",
    "    'email_address' : \"zbowyer@uw.edu\",         # your email address should go here\n",
    "    'access_token'  : access_token          # the access token you create will need to go here\n",
    "}\n",
    "\n",
    "#    This is a template of the data required as a payload when making a scoring request of the ORES model\n",
    "ORES_REQUEST_DATA_TEMPLATE = {\n",
    "    \"lang\":        \"en\",     # required that its english - we're scoring English Wikipedia revisions\n",
    "    \"rev_id\":      \"\",       # this request requires a revision id\n",
    "    \"features\":    True\n",
    "}\n",
    "\n",
    "def request_ores_score_per_article(article_revid = None, email_address=None, access_token=None,\n",
    "                                   endpoint_url = API_ORES_LIFTWING_ENDPOINT, \n",
    "                                   model_name = API_ORES_EN_QUALITY_MODEL, \n",
    "                                   request_data = ORES_REQUEST_DATA_TEMPLATE, \n",
    "                                   header_format = REQUEST_HEADER_TEMPLATE, \n",
    "                                   header_params = REQUEST_HEADER_PARAMS_TEMPLATE):\n",
    "    '''\n",
    "    Description:\n",
    "        Attempts to get ORES score from API call with supplied arguments.\n",
    "    Inputs:\n",
    "        article_revid - Integer\n",
    "        email_address - String\n",
    "        access_token - String\n",
    "        endpoint_url - String\n",
    "        model_name - String\n",
    "        request_data - Dictionary\n",
    "        header_format - Dictionary\n",
    "        header_params - Dictionary\n",
    "    Output:\n",
    "        Dictionary\n",
    "    '''\n",
    "    \n",
    "    # Make sure we have an article revision id, email and token\n",
    "    # This approach prioritizes the parameters passed in when making the call\n",
    "    if article_revid: request_data['rev_id'] = article_revid\n",
    "    if email_address: header_params['email_address'] = email_address\n",
    "    if access_token: header_params['access_token'] = access_token\n",
    "    \n",
    "    # Making a request requires a revision id - an email address - and the access token\n",
    "    if not request_data['rev_id']: raise Exception(\"Must provide an article revision id (rev_id) to score articles\")\n",
    "    if not header_params['email_address']: raise Exception(\"Must provide an 'email_address' value\")\n",
    "    if not header_params['access_token']: raise Exception(\"Must provide an 'access_token' value\")\n",
    "    \n",
    "    # Create the request URL with the specified model parameter - default is a article quality score request\n",
    "    request_url = endpoint_url.format(model_name=model_name)\n",
    "    \n",
    "    # Create a compliant request header from the template and the supplied parameters\n",
    "    headers = dict()\n",
    "    for key in header_format.keys():\n",
    "        headers[str(key)] = header_format[key].format(**header_params)\n",
    "\n",
    "    # make the request\n",
    "    try:\n",
    "        # we'll wait first, to make sure we don't exceed the limit in the situation where an exception\n",
    "        # occurs during the request processing - throttling is always a good practice with a free data\n",
    "        # source like ORES - or other community sources\n",
    "        if API_THROTTLE_WAIT > 0.0:\n",
    "            time.sleep(API_THROTTLE_WAIT)\n",
    "        response = requests.post(request_url, headers=headers, data=json.dumps(request_data))\n",
    "        json_response = response.json()\n",
    "    except Exception as e:\n",
    "        print(\"Exception\", e)\n",
    "        json_response = None\n",
    "    return json_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3cbe59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                             | 23/22157 [00:19<5:01:12,  1.22it/s]"
     ]
    }
   ],
   "source": [
    "# Approach to handling failures:\n",
    "#  If an ORES score for an article cannot be found,\n",
    "#  store the article name in a list and print out\n",
    "#  everything from the list at the end. Also store \n",
    "#  the score as N/A in the article list. We chose to print\n",
    "#  them all at the end to prevent print statements forcing the\n",
    "#  TQDM progress bar to be recreated. The output of the failures\n",
    "#  is simply a string for each article title.  \n",
    "\n",
    "# Step 2D: Get ORES score for each city article. \n",
    "# Step 2D1: Read each line of us_cities_by_state_SEPT.2023.csv\n",
    "unscored_articles = []\n",
    "for i in tqdm(range(len(CityArticles)), position=0, leave=True):\n",
    "    article_title = CityArticles[i][1]\n",
    "    article_url = CityArticles[i][2]\n",
    "    \n",
    "    # Step 2D2: Make a page info request to get the current article page revision\n",
    "    PageData = request_pageinfo_per_article(article_title)\n",
    "    pageid = list(PageData[\"query\"][\"pages\"].keys())[0]\n",
    "    revid = int(PageData[\"query\"][\"pages\"][pageid][\"lastrevid\"])\n",
    "    article_dict = {article_title: revid}\n",
    "    \n",
    "    # Step 2D3: Make an ORES request using the page title and current revision id.\n",
    "    score = request_ores_score_per_article(article_revid=revid, email_address=\"zbowyer@uw.edu\", access_token=access_token)\n",
    "    \n",
    "    # Step 2D4: Store score predictions if possible, else print out failures. \n",
    "    try:\n",
    "        ORES_prediction = score[\"enwiki\"][\"scores\"][str(revid)][\"articlequality\"][\"score\"][\"prediction\"]\n",
    "        CityArticles[i].append(ORES_prediction)\n",
    "        CityArticles[i].append(revid)\n",
    "    except:\n",
    "        CityArticles[i].append(\"N/A\")\n",
    "        unscored_articles.append(article_title)\n",
    "        CityArticles[i].append(revid)\n",
    "\n",
    "print(\"Articles that could not be scored: \")\n",
    "for x in unscored_articles: print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09b73ee",
   "metadata": {},
   "source": [
    "# Step 3: Combine datasets\n",
    "\n",
    "The goal here is to merge the wikipedia data and population data together.   \n",
    "This can be done because both datasets have state name fields.  \n",
    "Additionally regional-devisions must be added, so a third dataset will be combined.  \n",
    "\n",
    "All data with no matches should be logged.  \n",
    "\n",
    "#### Step 3A: Save article scores to a file separately for redundancy (it took a long time to generate). \n",
    "#### Step 3B: Convert all lists to dataframes\n",
    "#### Step 3C: Merge all dataframes on the 'state' column\n",
    "#### Step 3D: Save final merged dataframe to file\n",
    "#### Step 3E: Print out final merged dataframe to see if previous steps worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e09cd1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 3A: Save article scores to a file separately for redundancy (it took a long time to generate). \n",
    "#Uncomment on first passthrough\n",
    "#CityArticles_dataframe = pd.DataFrame(CityArticles, columns = ['state', 'article_title', 'url', 'article_quality'])\n",
    "#CityArticles_dataframe.to_csv(\"../data/CityArticles_withscores.csv\")\n",
    "CityArticles_dataframe = pd.read_csv(\"../data/CityArticles_withscores.csv\") #For reloading\n",
    "\n",
    "# Step 3B: Convert all lists to dataframes\n",
    "StateRegions_dataframe = pd.DataFrame(StateRegions, columns = ['region', 'regional_division', 'state'])\n",
    "StatePopEstimates_dataframe = pd.DataFrame(StatePopEstimates, columns = ['state', 'state_population'])\n",
    "RegionalPopEstimates_dataframe = pd.DataFrame(RegionalPopEstimates, columns = ['regional_division', 'region_population'])\n",
    "\n",
    "# Step 3C: Merge all dataframes on the 'state' column\n",
    "df3 = CityArticles_dataframe.merge(StatePopEstimates_dataframe, how='inner',left_on='state', right_on='state')\n",
    "df4 = df3.merge(StateRegions_dataframe, how='inner',left_on='state', right_on='state')\n",
    "df5 = df4.merge(RegionalPopEstimates_dataframe, how='inner', left_on='regional_division', right_on='regional_division')\n",
    "\n",
    "# Step 3D: Save final merged dataframe to file\n",
    "df5.to_csv(\"../data/wp_scored_city_articles_by_state.csv\")\n",
    "\n",
    "# Step 3E: Print out final merged dataframe to see if previous steps worked.\n",
    "print(df5.head(2))\n",
    "\n",
    "# Step 3F: Print out all non matched rows:\n",
    "# This indicates every spot where a join could not be made due to lack of data. \n",
    "print(\"-----------------------------------------------------------------\")\n",
    "print(\"Missing rows in combined dataframe!\")\n",
    "test_df = pd.merge(CityArticles_dataframe, StatePopEstimates_dataframe, how = 'left', left_on='state', right_on='state')\n",
    "missingrows1 = (test_df.loc[test_df['url'].isna()])\n",
    "missingrows2 = (test_df.loc[test_df['state_population'].isna()])\n",
    "\n",
    "test_df = pd.merge(df3, StateRegions_dataframe, how = 'left', left_on='state', right_on='state')\n",
    "missingrows3 = (test_df.loc[test_df['url'].isna()])\n",
    "missingrows4 = (test_df.loc[test_df['region'].isna()])\n",
    "\n",
    "test_df = pd.merge(df4, RegionalPopEstimates_dataframe, how = 'left', left_on='regional_division', right_on='regional_division')\n",
    "missingrows5 = (test_df.loc[test_df['url'].isna()])\n",
    "missingrows6 = (test_df.loc[test_df['region_population'].isna()])\n",
    "\n",
    "for x in missingrows1.iterrows(): print(x)\n",
    "for x in missingrows2.iterrows(): print(x[1][\"article_title\"])\n",
    "for x in missingrows3.iterrows(): print(x)\n",
    "for x in missingrows4.iterrows(): print(x)\n",
    "for x in missingrows5.iterrows(): print(x)\n",
    "for x in missingrows6.iterrows(): print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3091ede1",
   "metadata": {},
   "source": [
    "# Step 4: Analysis\n",
    "\n",
    "The goal here is to calculate metrics for our dataset. \n",
    "\n",
    "### For each state, and each division:\n",
    "    Total-articles-per-population (number of articles per person)  \n",
    "    High quality articles per population (number of high quality articles per person) (FA OR GA)    \n",
    "    \n",
    "#### Step 4A: Use groupby, merging, renaming, etc, to make calculated field of article number per capita by state\n",
    "#### Step 4B: Use groupby, merging, renaming, etc, to make calculated field of article number per capita by division\n",
    "#### Step 4C: Use groupby, merging, renaming, etc, to make calculated field of high quality article number per capita by state\n",
    "#### Step 4D: Use groupby, merging, renaming, etc, to make calculated field of high quality article number per capita by division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8475dd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4A: Use groupby, merging, renaming, etc, to make calculated field of article number per capita by state\n",
    "#Total articles per capita (STATE)\n",
    "States_num_articles = pd.DataFrame(df5.groupby('state').count()[\"article_title\"])\n",
    "States_num_articles = States_num_articles.rename(columns={\"article_title\": \"article_number\"})\n",
    "States_num_articles = States_num_articles.merge(StatePopEstimates_dataframe, how='inner',left_on='state', right_on='state')\n",
    "States_num_articles = States_num_articles.astype({'state_population': 'int32'})\n",
    "States_num_articles[\"article_number_per_capita\"] = States_num_articles[\"article_number\"] / States_num_articles[\"state_population\"]\n",
    "print(\"State-based number of articles per capita:\")\n",
    "print(States_num_articles.head(2))\n",
    "print(\"---------------------------------------------------------------------------------------------\")\n",
    "\n",
    "#### Step 4B: Use groupby, merging, renaming, etc, to make calculated field of article number per capita by division\n",
    "#Total articles per capita (Division)\n",
    "Divisions_num_articles = pd.DataFrame(df5.groupby('regional_division').count()[\"article_title\"])\n",
    "Divisions_num_articles = Divisions_num_articles.rename(columns={\"article_title\": \"article_number\"})\n",
    "Divisions_num_articles = Divisions_num_articles.merge(RegionalPopEstimates_dataframe, how='inner',left_on='regional_division', right_on='regional_division')\n",
    "Divisions_num_articles = Divisions_num_articles.astype({'region_population': 'int32'})\n",
    "Divisions_num_articles[\"article_number_per_capita\"] = Divisions_num_articles[\"article_number\"] / Divisions_num_articles[\"region_population\"]\n",
    "print(\"Division-based number of articles per capita:\")\n",
    "print(Divisions_num_articles.head(2))\n",
    "print(\"---------------------------------------------------------------------------------------------\")\n",
    "\n",
    "#Used for both 4C and 4D\n",
    "#Filter on only FA or GAs\n",
    "GA_or_FA = df5[df5['article_quality'].isin(['FA', 'GA'])]\n",
    "\n",
    "# Step 4C: Use groupby, merging, renaming, etc, to make calculated field of high quality article number per capita by state\n",
    "#Total high quality articles per capita (STATE)\n",
    "State_numquality_articles = GA_or_FA.groupby('state')['article_quality'].count().reset_index()\n",
    "State_numquality_articles = State_numquality_articles.merge(StatePopEstimates_dataframe, how='inner',left_on='state', right_on='state')\n",
    "State_numquality_articles = State_numquality_articles.rename(columns={\"article_quality\": \"article_number\"})\n",
    "State_numquality_articles = State_numquality_articles.astype({'state_population': 'int32'})\n",
    "State_numquality_articles[\"quality_article_number_per_capita\"] = State_numquality_articles[\"article_number\"] / State_numquality_articles[\"state_population\"]\n",
    "print(\"State-based number of quality articles per capita:\")\n",
    "print(State_numquality_articles.head(2))\n",
    "print(\"---------------------------------------------------------------------------------------------\")\n",
    "\n",
    "# Step 4D: Use groupby, merging, renaming, etc, to make calculated field of high quality article number per capita by division\n",
    "#Total high quality articles per capita (Division)\n",
    "Divisions_numquality_articles = GA_or_FA.groupby('regional_division')['article_quality'].count().reset_index()\n",
    "Divisions_numquality_articles = Divisions_numquality_articles.merge(RegionalPopEstimates_dataframe, how='inner',left_on='regional_division', right_on='regional_division')\n",
    "Divisions_numquality_articles = Divisions_numquality_articles.rename(columns={\"article_quality\": \"article_number\"})\n",
    "Divisions_numquality_articles = Divisions_numquality_articles.astype({'region_population': 'int32'})\n",
    "Divisions_numquality_articles[\"quality_article_number_per_capita\"] = Divisions_numquality_articles[\"article_number\"] / Divisions_numquality_articles[\"region_population\"]\n",
    "print(\"Division-based number of quality articles per capita:\")\n",
    "print(Divisions_numquality_articles.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974819e3",
   "metadata": {},
   "source": [
    "# Step 5: Results\n",
    "\n",
    "#### The goal here is to produce six tables that show:\n",
    "5A. Top 10 US states by coverage: The 10 US states with the highest total articles per capita (in descending order).  \n",
    "5B. Bottom 10 US states by coverage: The 10 US states with the lowest total articles per capita (in ascending order).  \n",
    "5C. Top 10 US states by high quality: The 10 US states with the highest high quality articles per capita (in descending order).  \n",
    "5D. Bottom 10 US states by high quality: The 10 US states with the lowest high quality articles per capita (in ascending order).  \n",
    "5E. Census divisions by total coverage: A rank ordered list of US census divisions (in descending order) by total articles per capita.  \n",
    "5F. Census divisions by high quality coverage: Rank ordered list of US census divisions (in descending order) by high quality articles per capita.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4e79bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5A: Top 10 US states by coverage: The 10 US states with the highest total articles per capita (in descending order).  \n",
    "States_num_articles.sort_values(by='article_number_per_capita', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdd4139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5B: Bottom 10 US states by coverage: The 10 US states with the lowest total articles per capita (in ascending order).  \n",
    "States_num_articles.sort_values(by='article_number_per_capita', ascending=True).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac846d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5C: Top 10 US states by high quality: The 10 US states with the highest number of high quality articles per capita (in descending order).\n",
    "State_numquality_articles.sort_values(by='quality_article_number_per_capita', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1c1e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5D: Bottom 10 US states by high quality: The 10 US states with the lowest high quality articles per capita (in ascending order).\n",
    "State_numquality_articles.sort_values(by='quality_article_number_per_capita', ascending=True).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91d4ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5E: Census divisions by total coverage: A rank ordered list of US census divisions (in descending order) by total articles per capita.\n",
    "Divisions_num_articles.sort_values(by='article_number_per_capita', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a91ec80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5F Census divisions by high quality coverage: Rank ordered list of US census divisions (in descending order) by high quality articles per capita.\n",
    "Divisions_numquality_articles.sort_values(by='quality_article_number_per_capita', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94590f88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
