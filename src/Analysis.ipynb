{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b775f10",
   "metadata": {},
   "source": [
    "# This notebook serves as the executable software that reads in datasets, combines datasets, and runs data analysis\n",
    "\n",
    "The end goal is to perform analysis on how the coverage of US cities on wikipedia and how the quality of articles about cities varies among states. \n",
    "\n",
    "Steps: \n",
    "1. Confirm datasets are ready to go.\n",
    "1. Combine dataset of wikipedia articles with a dataset of state populations\n",
    "2. Use ORES to estiamte quality of articles about the cities\n",
    "3. Data analyis\n",
    "   3a. The states with the greatest and least coverage of cities on Wikipedia compared  \n",
    "       to their population.\n",
    "   3b. The states with the highest and lowest proportion of high quality articles about cities.\n",
    "   3c. A ranking of US geographic regions by articles-per-person and proportion of high \n",
    "       quality articles.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827461a8",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "96e493de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import json\n",
    "import time\n",
    "import urllib.parse\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de76a5a4",
   "metadata": {},
   "source": [
    "# Part 1. Confirm datasets are ready to go. \n",
    "\n",
    "Confirm that the files below are there:  \n",
    "./data/PopulationEstimates.csv   \n",
    "./data/States_by_region.csv  \n",
    "./data/us_cities_by_state_SEPT2023.csv  \n",
    "\n",
    "#### Step 1A: Read in ./data/PopulationEstimates.csv as csv object.\n",
    "    ##### Step 1A1: Store only state and population data from csv into list for just states. \n",
    "    ####  Step 1A2: Output first, middle, and last row of csv object to confirm data is in memory. \n",
    "#### Step 1B: Read in ./data/States_by_region.csv as csv object.\n",
    "    ##### Step 1B1: Store all data from csv into list. \n",
    "    ##### Step 1B2: Remove header row from list to prevent it showing up later.    \n",
    "    ##### Step 1B3: Output first, middle, and last row of csv object to confirm data is in memory. \n",
    "#### Step 1C: Read in ./data/us_cities_by_state_SEPT2023.csv as csv object.\n",
    "    ##### Step 1C1: Store all data from csv into list. \n",
    "    ##### Step 1C2: Remove header row from list to prevent it showing up later.    \n",
    "    ##### Step 1C3: Output first, middle, and last row of csv object to confirm data is in memory. \n",
    "\n",
    "### Data sources: \n",
    "##### ./data/PopulationEstimates.csv  \n",
    "https://www2.census.gov/programs-surveys/popest/datasets/2020-2022/state/totals/NST-EST2022-ALLDATA.csv    \n",
    "https://www.census.gov/data/tables/time-series/demo/popest/2020s-state-total.html  \n",
    "##### ./data/States_by_region.csv    \n",
    "The shared google drive for Homework 2 by UW Data 512 provides this file.   \n",
    "##### ./data/us_cities_by_state_SEPT2023.csv  \n",
    "The shared google drive for Homework 2 by UW data 512 provies this file.   \n",
    "However the Wikipedia Category:Lists of cities in the United States by   \n",
    "state was crawled to generate a list of Wikipedia article pages about US  \n",
    "cities from each state.   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a96d63df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of states, should be 50: 50\n",
      "['Alabama', '5074296']\n",
      "['Montana', '1122867']\n",
      "['Wyoming', '581381']\n"
     ]
    }
   ],
   "source": [
    "# Step 1A: Read in ./data/PopulationEstimates.csv as csv object.\n",
    "StatePopEstimates = []\n",
    "with open('../data/PopulationEstimates.csv', newline='') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "    \n",
    "    # Step 1A1: Store only state and population data from csv into list for just states. \n",
    "    #  This means excluding the first 15 and the last rows, which correspond to \n",
    "    #  the header row, larger regions of the United States, and Puerto Rico, which are not states. \n",
    "    counter = 0\n",
    "    for row in reader: \n",
    "        if(counter > 14 and counter < 66):\n",
    "            StatePopEstimates.append([row[4], row[8]]) \n",
    "        counter += 1\n",
    "        \n",
    "# 1A1: Remove district of columbia\n",
    "StatePopEstimates.pop(8)\n",
    "        \n",
    "# Step 1A2: Output first, middle, and last row of csv object to confirm data is in memory.  \n",
    "print(\"Number of states, should be 50:\", len(StatePopEstimates))\n",
    "print(StatePopEstimates[0])\n",
    "print(StatePopEstimates[len(StatePopEstimates)//2])\n",
    "print(StatePopEstimates[len(StatePopEstimates)-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "dc0f8376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of states, should be 50: 50\n",
      "['Northeast', 'New England', 'Connecticut']\n",
      "['South', 'South Atlantic', 'North Carolina']\n",
      "['West', 'Pacific', 'Washington']\n"
     ]
    }
   ],
   "source": [
    "# Step 1B: Read in ./data/States_by_region.csv as csv object.\n",
    "StateRegions = []\n",
    "with open('../data/States_by_region.csv', newline='') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "    \n",
    "    # Step 1B1: Store all data from csv into list. \n",
    "    for row in reader: \n",
    "        StateRegions.append([row[0], row[1], row[2]]) \n",
    "\n",
    "# Step 1B2: Remove header row\n",
    "StateRegions.pop(0)\n",
    "\n",
    "# Step 1B3: Output first, middle, and last row of csv object to confirm data is in memory.\n",
    "print(\"Number of states, should be 50:\", len(StateRegions))\n",
    "print(StateRegions[0])\n",
    "print(StateRegions[len(StateRegions)//2])\n",
    "print(StateRegions[len(StateRegions)-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "581a6e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Alabama', 'Abbeville, Alabama', 'https://en.wikipedia.org/wiki/Abbeville,_Alabama']\n",
      "['Minnesota', 'Sargeant, Minnesota', 'https://en.wikipedia.org/wiki/Sargeant,_Minnesota']\n",
      "['Wyoming', 'Yoder, Wyoming', 'https://en.wikipedia.org/wiki/Yoder,_Wyoming']\n"
     ]
    }
   ],
   "source": [
    "#### Step 1C: Read in ./data/us_cities_by_state_SEPT2023.csv as csv object.\n",
    "CityArticles = []\n",
    "with open('../data/us_cities_by_state_SEPT2023.csv', newline='', encoding=\"utf8\") as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "    \n",
    "    # Step 1C1: Store all data from csv into list. \n",
    "    for row in reader: \n",
    "        CityArticles.append([row[0], row[1], row[2]]) \n",
    "\n",
    "# Step 1C2: Remove header row\n",
    "CityArticles.pop(0)\n",
    "\n",
    "# Step 1C3: Output first, middle, and last row of csv object to confirm data is in memory.\n",
    "print(CityArticles[0])\n",
    "print(CityArticles[len(CityArticles)//2])\n",
    "print(CityArticles[len(CityArticles)-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5926aa25",
   "metadata": {},
   "source": [
    "# Part 2. Get Article Quality Predictions\n",
    "We're using a machine learning system called ORES. This was originally an acronym for \"Objective Revision Evaluation Service\" but was simply renamed “ORES”. ORES is a machine learning tool that can provide estimates of Wikipedia article quality. The article quality estimates are, from best to worst:\n",
    "FA - Featured article\n",
    "GA - Good article (sometimes called A-class)\n",
    "B - B-class article\n",
    "C - C-class article\n",
    "Start - Start-class article\n",
    "Stub - Stub-class article\n",
    "These labelings were learned based on articles in Wikipedia that were peer-reviewed using the Wikipedia content assessment procedures.These quality classes are a subset of quality assessment categories developed by Wikipedia editors.\n",
    "\n",
    "#### Step 2A: Create wikimedia user account to generate API token. \n",
    "    ##### https://api.wikimedia.org/w/index.php?title=Special:UserLogin \n",
    "#### Step 2A: Define constants to make data requests.\n",
    "    ##### Step 2A1: \n",
    "    ####  Step 2A2: \n",
    "#### Step 2B: ?\n",
    "#### Step 2C: ?\n",
    "#### Step 2D: ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "59706cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code in this cell was developed by Dr. David W. McDonald for use in DATA 512, \n",
    "# a course in the UW MS Data Science degree program. \n",
    "# This code is provided under the Creative Commons CC-BY license. Revision 1.1 - August 14, 2022\n",
    "# Any modifications made to the original source also fall under the CC-BY license. \n",
    "\n",
    "# The basic English Wikipedia API endpoint\n",
    "API_ENWIKIPEDIA_ENDPOINT = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "# We'll assume that there needs to be some throttling for these requests - we should always be nice to a free data resource\n",
    "API_LATENCY_ASSUMED = 0.002       # Assuming roughly 2ms latency on the API and network\n",
    "API_THROTTLE_WAIT = (1.0/100.0)-API_LATENCY_ASSUMED\n",
    "\n",
    "# When making automated requests we should include something that is unique to the person making the request\n",
    "# This should include an email - your UW email would be good to put in there\n",
    "REQUEST_HEADERS = {\n",
    "    'User-Agent': '<uwnetid@uw.edu>, University of Washington, MSDS DATA 512 - AUTUMN 2023',\n",
    "}\n",
    "\n",
    "# This is just a list of English Wikipedia article titles that we can use for example requests\n",
    "ARTICLE_TITLES = [ 'Bison', 'Northern flicker', 'Red squirrel', 'Chinook salmon', 'Horseshoe bat' ]\n",
    "\n",
    "# This is a string of additional page properties that can be returned see the Info documentation for\n",
    "# what can be included. If you don't want any this can simply be the empty string\n",
    "PAGEINFO_EXTENDED_PROPERTIES = \"talkid|url|watched|watchers\"\n",
    "#PAGEINFO_EXTENDED_PROPERTIES = \"\"\n",
    "\n",
    "# This template lists the basic parameters for making this\n",
    "PAGEINFO_PARAMS_TEMPLATE = {\n",
    "    \"action\": \"query\",\n",
    "    \"format\": \"json\",\n",
    "    \"titles\": \"\",           # to simplify this should be a single page title at a time\n",
    "    \"prop\": \"info\",\n",
    "    \"inprop\": PAGEINFO_EXTENDED_PROPERTIES\n",
    "}\n",
    "\n",
    "def request_pageinfo_per_article(article_title = None, \n",
    "                                 endpoint_url = API_ENWIKIPEDIA_ENDPOINT, \n",
    "                                 request_template = PAGEINFO_PARAMS_TEMPLATE,\n",
    "                                 headers = REQUEST_HEADERS):\n",
    "    \n",
    "    # article title can be as a parameter to the call or in the request_template\n",
    "    if article_title:\n",
    "        request_template['titles'] = article_title\n",
    "\n",
    "    if not request_template['titles']:\n",
    "        raise Exception(\"Must supply an article title to make a pageinfo request.\")\n",
    "\n",
    "    # make the request\n",
    "    try:\n",
    "        # we'll wait first, to make sure we don't exceed the limit in the situation where an exception\n",
    "        # occurs during the request processing - throttling is always a good practice with a free\n",
    "        # data source like Wikipedia - or any other community sources\n",
    "        if API_THROTTLE_WAIT > 0.0:\n",
    "            time.sleep(API_THROTTLE_WAIT)\n",
    "        response = requests.get(endpoint_url, headers=headers, params=request_template)\n",
    "        json_response = response.json()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        json_response = None\n",
    "    return json_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e23f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code in this cell was developed by Dr. David W. McDonald for use in DATA 512, \n",
    "# a course in the UW MS Data Science degree program. \n",
    "# This code is provided under the Creative Commons CC-BY license. Revision 1.1 - August 15, 2023\n",
    "# Any modifications made to the original source also fall under the CC-BY license. \n",
    "\n",
    "#    The current LiftWing ORES API endpoint and prediction model\n",
    "#\n",
    "API_ORES_LIFTWING_ENDPOINT = \"https://api.wikimedia.org/service/lw/inference/v1/models/{model_name}:predict\"\n",
    "API_ORES_EN_QUALITY_MODEL = \"enwiki-articlequality\"\n",
    "\n",
    "#\n",
    "#    The throttling rate is a function of the Access token that you are granted when you request the token. The constants\n",
    "#    come from dissecting the token and getting the rate limits from the granted token. An example of that is below.\n",
    "#\n",
    "API_LATENCY_ASSUMED = 0.002       # Assuming roughly 2ms latency on the API and network\n",
    "API_THROTTLE_WAIT = (60.0/5000.0)-API_LATENCY_ASSUMED\n",
    "\n",
    "#    When making automated requests we should include something that is unique to the person making the request\n",
    "#    This should include an email - your UW email would be good to put in there\n",
    "#    \n",
    "#    Because all LiftWing API requests require some form of authentication, you need to provide your access token\n",
    "#    as part of the header too\n",
    "#\n",
    "REQUEST_HEADER_TEMPLATE = {\n",
    "    'User-Agent': \"<{zbowyer@uw.edu}>, University of Washington, MSDS DATA 512 - AUTUMN 2023\",\n",
    "    'Content-Type': 'application/json',\n",
    "    'Authorization': \"Bearer {access_token}\"\n",
    "}\n",
    "#\n",
    "#    This is a template for the parameters that we need to supply in the headers of an API request\n",
    "#\n",
    "REQUEST_HEADER_PARAMS_TEMPLATE = {\n",
    "    'email_address' : \"\",         # your email address should go here\n",
    "    'access_token'  : \"\"          # the access token you create will need to go here\n",
    "}\n",
    "\n",
    "#\n",
    "#    A dictionary of English Wikipedia article titles (keys) and sample revision IDs that can be used for this ORES scoring example\n",
    "#\n",
    "ARTICLE_REVISIONS = { 'Bison':1085687913 , 'Northern flicker':1086582504 , 'Red squirrel':1083787665 , 'Chinook salmon':1085406228 , 'Horseshoe bat':1060601936 }\n",
    "\n",
    "#\n",
    "#    This is a template of the data required as a payload when making a scoring request of the ORES model\n",
    "#\n",
    "ORES_REQUEST_DATA_TEMPLATE = {\n",
    "    \"lang\":        \"en\",     # required that its english - we're scoring English Wikipedia revisions\n",
    "    \"rev_id\":      \"\",       # this request requires a revision id\n",
    "    \"features\":    True\n",
    "}\n",
    "\n",
    "#\n",
    "#    These are used later - defined here so they, at least, have empty values\n",
    "#\n",
    "USERNAME = \"\"\n",
    "ACCESS_TOKEN = \"\"\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "5df30321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"batchcomplete\": \"\",\n",
      "    \"query\": {\n",
      "        \"pages\": {\n",
      "            \"1212891\": {\n",
      "                \"pageid\": 1212891,\n",
      "                \"ns\": 0,\n",
      "                \"title\": \"Chinook salmon\",\n",
      "                \"contentmodel\": \"wikitext\",\n",
      "                \"pagelanguage\": \"en\",\n",
      "                \"pagelanguagehtmlcode\": \"en\",\n",
      "                \"pagelanguagedir\": \"ltr\",\n",
      "                \"touched\": \"2023-10-09T04:12:00Z\",\n",
      "                \"lastrevid\": 1178125499,\n",
      "                \"length\": 49187,\n",
      "                \"watchers\": 102,\n",
      "                \"talkid\": 3909817,\n",
      "                \"fullurl\": \"https://en.wikipedia.org/wiki/Chinook_salmon\",\n",
      "                \"editurl\": \"https://en.wikipedia.org/w/index.php?title=Chinook_salmon&action=edit\",\n",
      "                \"canonicalurl\": \"https://en.wikipedia.org/wiki/Chinook_salmon\"\n",
      "            }\n",
      "        }\n",
      "    }\n",
      "}\n",
      "1178125499\n"
     ]
    }
   ],
   "source": [
    "# a) read each line of us_cities_by_state_SEPT.2023.csv\n",
    "for i in range(len(CityArticles)):\n",
    "    article_url = CityArticles[i][2]\n",
    "    \n",
    "    # b) make a page info request to get the current article page revision\n",
    "    PageData = request_pageinfo_per_article(ARTICLE_TITLES[3])\n",
    "    pageid = list(PageData[\"query\"][\"pages\"].keys())[0]\n",
    "    revid = PageData[\"query\"][\"pages\"][pageid][\"lastrevid\"]\n",
    "    \n",
    "    # c) then  make an ORES request using the page title and current revision id.\n",
    "    \n",
    "    \n",
    "    print(json.dumps(PageData,indent=4))\n",
    "    break\n",
    "\n",
    "# d) maintain a log of articles for which you were not able to retrieve an ORES score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06c8f78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
